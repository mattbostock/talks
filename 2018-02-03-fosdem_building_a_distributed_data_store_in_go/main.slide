Designing and building a distributed data store in Go
03 Feb 2018
Tags: golang, timbala, prometheus, tsdb, distributed systems, hashring, consistent hashing, pprof

Matt Bostock
@mattbostock

* Who am I?

Platform Engineer working for Cloudflare in London.

Interested in distributed systems and performance.

: Organise the Prometheus London meetup
: Until September, studying a Masters in Computer Science at night school. Hard but super interesting.

* Bulding and designing a distributed data store

* What I will (and won't) cover in this talk

MSc Computer Science final project

: I passed!

* Timbala

.image timbala_logo_horizontal.svg

: Project was called AthensDB, now renamed to Timbala
: this is the second logo
: distributed time-series database

: Distributed systems is a huge topic, I won't be able to explain everything but hopefully you'll take something away
: This is really a talk about how I approached this problem, with some leads on techniques/algorithms you might use

* It ain't production-ready

Please, please, don't use it yet in Production if you care about your data.

* What's 'distributed'?

: coordination between networked computers

"A distributed system is a model in which components located on networked computers communicate and coordinate their actions by passing messages."

-- Wikipedia

: the coordination part is really hard, as we'll find out

* Why distributed?

Survive the failure of individual servers

Add more servers to meet demand

: horizontal scaling

* Fallacies of distributed computing

: list that came out of Sun Microsystems

The network is reliable.
Latency is zero.
Bandwidth is infinite.
The network is secure.
Topology doesn't change.
There is one administrator.
Transport cost is zero.
The network is homogeneous.

: makes building this kind of system interesting

* Use case

Durable long-term storage for metrics

: time-series database capable of reliably storing multidimensional
: metrics over a period of 10 years or more and capable of storing more metrics
: than can be accommodated by a single commodity server.

* Why not use 'the Cloud'?

: Wouldn't make for a good master's project

- On-premise, mid-sized deployments

: don't necessarily want the hassle of operating Hadoop or Ceph

- High performance, low latency
- Ease of operation

* Requirements

* Sharding

: sharding == spreading data across multiple servers by splitting it up into chunks called shards

The database must be able to store more data than could fit on a single node.

* Replication

The system must replicate data across multiple nodes to prevent data loss when
individual nodes fail.

: replicas are copies of the data, in case we lose a copy

* High availability and throughput for data ingestion

Must be able to store a lot of data, reliably

: humans can retry queries
: but we don't want to delay data from being stored in the system as we'll be querying outdated data and it'll backlog (back pressure)

* Operational simplicity

: it must be simple to operate and maintain
: keep configuration options to a minimum (less to get wrong)
: good instrumentation (logging, metrics and tracing)

* Interoperability with Prometheus

Reuse Prometheus' best features

: including the query language (PromQL), APIs, and data model.

Avoid writing my own query language and designing my own APIs

Focus on the 'distributed' part

: in itself hard enough

* By the numbers

: to help frame the problem, I looked at our own usage for long-term metrics storage

Cloudflare's OpenTSDB installation (mid-2017):

- 700k data points per second
- 70M unique timeseries

* Minimum Viable Product (MVP)?

: hard to determine
: where do you start?
: helped to think about ingestion versus querying (read versus write paths)

* How to reduce the scope?

: what parts can I avoid doing myself?

Reuse third-party code wherever possible

: PromQL library; API code

* Milestone 1: Single-node implementation

: storage is going to be hard, let's get it working for one node

Ingestion API

: get data in

Query API

: get data out

Local, single node, storage

* Milestone 2: Clustered implementation

1. Shard data between nodes (no replication yet)

2. Replicate shards

3. Replication rebalancing using manual intervention

* Beyond a minimum viable product

Read repair

: As you're reading data, compare results across nodes and fill in the gaps as they're spotted

Hinted handoff

: Temporarily store data on behalf of other nodes that may be temporarily failing

Active anti-entropy

: Asynchronous (background) process that compares data between nodes and fills in the gaps
: Could also be used for automatic rebalancing

* To the research!

: First thing I did was read a ton
: Good news is there's a wealth of good material of distributed systems and how to optimise them
: Bad news is, most of these things won't help me get started

NUMA
Data/cache locality
SSDs
Write amplification
Alignment with disk storage, memory pages
mmap(2)
Jepsen testing
Formal verification methods
Bitmap indices
xxHash, City hash, Murmur hash, Farm hash, Highway hash

: Nice to have, but need to start small

* Back to the essentials

: consensus, or lack of

: cap theorem

Coordination

: OK log

Indexing

: AKA how do you find the data quickly? also on other nodes in the cluster, not just the local node

On-disk storage format

: efficient storage

Cluster membership

: how do you know which servers are online? when to determine that they've failed?

Data placement (replication/sharding)

: how to determine where data should go, so that it can be written and accessed quickly?

Failure modes

: how many replicas should we write before considering the data to be 'safe'?
: how many replicas do we need to consider we have enough data to successfully serve a read query?

* Traits (or assumptions) of time-series data

: let's try to understand the problem space a bit better

* Immutable data

No updates to existing data!

: good news folks
: simplifies the system substantially

No need to worry about managing multiple versions of the same value and copying (replicating) them between servers

: relaxes our requirements for consistency
: makes for an easier introduction to a distributed system

* Simple data types; compress well

Don't need to worry about arrays or strings

: just numbers
: some TSDBs do store non-numeric data, e.g. InfluxDB

Double-delta compression for floats

: see Facebook's Gorilla paper

.link http://www.vldb.org/pvldb/vol8/p1816-teller.pdf Gorilla: A Fast, Scalable, In-Memory Time Series Database

# TODO give example?

* Tension between write and read patterns

Continous writes across majority of individual time-series

Occasional reads for small subsets of time-series across historical data

# TODO add diagram

: problem consists of trying to demultiplexing (or splitting) writes into files that allow for fast queries without accessing all timeseries

.link https://fabxc.org/tsdb/ Writing a Time Series Database from Scratch

* Prior art

: I won't go into detail now as I wouldn't be able to them justice, but these designs had a lot of influence on my design

Amazon's Dynamo paper

: not the same as Amazon DynamoDB

Apache Cassandra

Basho Riak

: use of consistent hashing to determine data placement within the cluster
: mechanisms for repairing data (active anti-entropy, read repair and hinted handoff as we'll discuss in a moment)

Google BigTable

: columnar storage

Other time-series databases

: OpenTSDB, InfluxDB, DalmatinerDB, Facebook's Gorilla, and more

* Coordination

: oklog article
: helped to frame the problem
: I was too focused on consensus at the start even though I didn't yet know what state should be shared between nodes in the cluster

Keep coordination to a minimum

: it's hard to do correctly, each server has its own view of the world

Avoid coordination bottlenecks

: updating a centralised or syncrhonous index on every write would be a bottleneck on performance

* Cluster membership

Need to know which nodes are in the cluster at any given time

Could be static, dynamic is preferable

Need to know when a node is dead so we can stop using it

* Memberlist library

I used Hashicorp's Memberlist library

: really easy to use

Used by Serf and Consul

SWIM gossip protocol

: the nodes in the cluster 'gossip' to each other over UDP
: occasional 'reliable' sync using TCP
: nodes that don't respond for a given grace period are considered dead
: but nodes can be checked indirectly, meaning nodes can 'snitch' on the status of other nodes they're connected to

* Indexing

: AKA how do you find the data quickly? also on other nodes in the cluster, not just the local node

* Could use a centralised index

Consistent view; knows where each piece of data should reside

Index needs to be replicated in case a node fails

Likely to become a bottleneck at high ingestion volumes

Needs coordination, possibly consensus

: could be asynchonous and eventually consistent given that metrics are immutable

* Could use a local index

Each node knows what data it has

: but how to know what other nodes have?
: look at prior art, maybe consistent hashing similar to how Dynamo, Cassandra and Riak work?

* Data placement (replication/sharding)

: how to determine where data should go, so that it can be written and accessed quickly?

* Consistent hashing

: decided to use consistent hashing

Hashing uses maths to put items into buckets

: in this case, those buckets are servers

Consistent hashing aims to keep disruption to a minimum when the number of buckets changes

: in our case, we want to minimise how much data moves to a different server when the number of servers in a cluster changes

* Consistent hashing: example

n = nodes in the cluster

1/n of data should be displaced/relocated when a single node fails

Example:

- 5 nodes
- 1 node fails
- one fifth of data needs to move

* Consistent hashing algorithms

: looked at a bunch, see the decision record

.link https://github.com/mattbostock/timbala/issues/27 Decision record for determining consistent hashing algorithm

* Consistent hashing algorithms

: I won't go into detail

First attempt: Karger et al (Akamai) algorithm

.link https://www.akamai.com/es/es/multimedia/documents/technical-publication/consistent-hashing-and-random-trees-distributed-caching-protocols-for-relieving-hot-spots-on-the-world-wide-web-technical-publication.pdf Karger et al paper
.link https://github.com/golang/groupcache/blob/master/consistenthash/consistenthash.go

Second attempt: Jump hash

.link https://arxiv.org/abs/1406.2294 Jump hash paper
.link https://github.com/dgryski/go-jump/blob/master/jump.go

* Jump hash implementation

: check out the reference implementation, it's like 9 lines of code
: uses a magic constant to generate pseudo-random numbers
: AKA 64-bit Linear Congruential Generator
: not to hard to understand if you put aside the performance optimisations

  func Hash(key uint64, numBuckets int) int32 {
          var b int64 = -1
          var j int64

          for j < int64(numBuckets) {
                  b = j
                  key = key*2862933555777941757 + 1
                  j = int64(float64(b+1) * (float64(int64(1)<<31) / float64((key>>33)+1)))
          }

          return int32(b)
  }

# License for jump hash implementation
#
# The MIT License (MIT)
# 
# Copyright (c) 2014 Damian Gryski <damian@gryski.com>
# 
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
# 
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
# 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.

.link https://github.com/dgryski/go-jump/blob/master/jump.go

* Partition key

#- choice of partition key

The hash function needs some input

: here we call the input the partition key

The partition key influences which bucket data is placed in

: really important in how it impacts on how many nodes are involved in reads and writes

.link https://github.com/mattbostock/timbala/issues/12 Decision record for partition key

* Replicas

3 replicas (copies) of each shard

Achieved by prepending the replica number to the partition key

* On-disk storage format

: looked at

Log-structured merge
LevelDB
RocksDB
LMDB
B-trees and b-tries (bitwise trie structure) for indexes
Locality-preserving hashes

: useful for multi-dimensional data?

: storage is hard
: around the same time, the one of the Prometheus developers started work on a new storage engine

* Use an existing library

.link https://github.com/prometheus/tsdb Prometheus TSDB library

Cleaner interface than previous Prometheus storage engine

Intended to be used as a library

: easier to integrate

: see Fabian's talk
: conclusion: good programmers are lazy programmers

.link https://fabxc.org/tsdb/ Writing a Time Series Database from Scratch

#* Failure modes

* Architecture

# what coordination is required?
No centralised index (only shared state is node metadata)

Each node has the same role

Any node can receive a query

Any node can receive new data

: New data will be routed to the appropriate nodes
: Ingestion and querying should be load-balanced between nodes

No centralised index, data placement is determined by consistent hash

* Testing

- Unit tests
- Acceptance tests
- Integration tests
- Benchmarking

: aiming to get a high degree of value from the tests at this stage in development
: majority of tests, including the integration tests, have the Go race detector enabled
: found that the race detector doesn't work with musl libc under Alpine Linux

* Unit tests

* Data distribution tests

How even is the distribution of samples across nodes in the cluster?

: want to avoid a single node from storing more than others

Are replicas of the same data stored on separate nodes?

: I ensured this by shifting a replica to the next unused node if a replica
: already exists on that node

* 

: uses subtests to test varying replica sizes across varying cluster sizes

  === RUN TestHashringDistribution/3_replicas_across_5_nodes
  Distribution of samples when replication factor is 3 across a cluster of 5 nodes:
  Node 0 : #########                                          19.96%; 59891 samples
  Node 1 : #########                                          19.99%; 59967 samples
  Node 2 : ##########                                         20.19%; 60558 samples
  Node 3 : #########                                          19.74%; 59212 samples
  Node 4 : ##########                                         20.12%; 60372 samples
  Summary:
  Min: 59212
  Max: 60558
  Mean: 60000.00
  Median: 59967
  Standard deviation: 465.55
  Total samples: 300000

  Distribution of 3 replicas across 5 nodes:
  0 nodes:                                                    0.00%; 0 samples
  1 nodes:                                                    0.00%; 0 samples
  2 nodes:                                                    0.00%; 0 samples
  3 nodes: ################################################## 100.00%; 100000 samples

  Replication summary:
  Min nodes samples are spread over: 3
  Max nodes samples are spread over: 3
  Mode nodes samples are spread over: [3]
  Mean nodes samples are spread over: 3.00

* Data displacement tests

If I change the cluster size, how much data needs to move servers?

: test the hashring with unit tests, add a bunch of samples, change the cluster size
: count how many samples were hashed to a different server

  === RUN   TestHashringDisplacement
  293976 unique samples
  At most 19598 samples should change node
  15477 samples changed node

  293976 unique samples
  At most 21776 samples should change node
  16199 samples changed node
  --- PASS: TestHashringDisplacement (4.33s)

* Data displacement failure

: this test identified an bug

Too much data was being moved because I was sorting the list of nodes alphabetically

: aiming for deterministic behaviour
: but worked against the Jumphash consistent hashing algorithm

* Jump hash gotcha 

: this sentence in the paper caught me out

"Its main limitation is that the buckets must be numbered sequentially, which makes it
more suitable for data storage applications than for distributed web caching."

: Numbered sequentially is important

Jump hash works on buckets, not server names

Conclusion: Each node needs to remember the order in which it joined the cluster

* Acceptance tests

Verify core functionality from a user perspective

: simple arithmetic in promql queries sent to HTTP query API
: sending samples to remote write api
: writing data then querying it back, comparing the results
: checking for presence of debug endpoints and metrics (important for operations)
: run the application binary, allows for testing command-line flags


* Integration tests

Most effective, least brittle tests at this stage in the project

: not affected as much by code refactoring

Some cross-over with acceptance tests

: acceptance tests focused on a single node (run quickly, verify APIs)
: integration tests focused on interoperability with third-party components such as Prometheus server and client libraries
: integration tests also test the clustering functionality

Docker compose for portability, easy to define

: found race conditions
: tested Prometheus as a client sending data to the remote write API
: tested Prometheus official client library worked with Timbala API (should be compatible)
: tested interoperability with nginx though later removed (one issue found)

* 

: Integration tests run in Travis CI on every PR

.background travis_integration_tests.png
#.image travis_integration_tests.png _ 1200

: three-node cluster running in Docker Compose on Travis CI

* Benchmarking

Benchmarking harness using Docker Compose

* 
.background measurements.png
#.image measurements.png _ 800

: Allowed for debugging the running processes under load

* pprof

: pprof profiling
: no-ops to benchmark parts of the system; pprof

  go tool pprof

or

: more up-to-date

  go get github.com/google/pprof

.link https://tip.golang.org/doc/diagnostics.html Go Diagnostics

* pprof CPU profile

  pprof --dot http://localhost:9080/debug/pprof/profile | dot -T png | open -f -a /Applications/Preview.app

* 

: generated output that looks like this:

.background cpu_profile_label_hash.png
#.image cpu_profile_label_hash.png _ 1000

: noticed that I was hashing metrics labels inside a loop
: moved the hashing outside the loop
: can also reveal too much time spent in garbage collection
: and memory allocations

: take out bits of code, turn those code paths into no-ops
: e.g. network requests, to see efficiency of other code paths
: beware of premature optimisation

* Gauging the impact of garbage collection

: GOGC to turn off garbage collection, see what impact it's having

  GOGC=off

.link https://golang.org/pkg/runtime/

# Randomly generated metrics, using a seed so tests can be repeated deterministically

: multiple workers sending generated metrics

* Microbenchmarks

: using Go testing library

  $ go test -benchmem -bench BenchmarkHashringDistribution -run none ./internal/cluster
  goos: darwin
  goarch: amd64
  pkg: github.com/mattbostock/timbala/internal/cluster
  BenchmarkHashringDistribution-4   	 2000000	       954 ns/op	     544 B/op	       3 allocs/op
  PASS
  ok  	github.com/mattbostock/timbala/internal/cluster	3.303s

.link https://golang.org/pkg/testing/#hdr-Benchmarks

* Failure injection

Stop nodes

: Give additional privileges to a control container that can tell Docker to stop/start nodes in the cluster

Packet loss, re-ordering, latency using tc (Traffic Control)

: add NET_ADMIN capability to control container to manipulate Docker network

.link https://www.qualimente.com/2016/04/26/introduction-to-failure-testing-with-docker/

* Conclusions 

- Greatest challenge in distribution systems is anticipating how they will fail and lose data

: note will, not if

- Make sure you understand the tradeoffs your Production systems are making

: primary learning from the project that the greatest challenge when implementing
: a distributed system is not so much the implementation (known knowns or known unknowns)
: but rather the more insidious difficulty of reasoning about its failure modes and the potential
: combination of factors that could lead to data loss.

: distributed systems are fascinating, would encourage people to try writing their own
: make sure you understand the tradeoffs your Production systems are making

* Use dep, it's awesome 🤘

: tool for managing dependencies
: my best experience with managing Go dependencies

.link https://github.com/golang/dep

* More information

.link https://github.com/mattbostock/timbala/blob/master/docs/architecture.md Timbala architecture documentation
.link https://dataintensive.net/ Designing Data-Intensive Systems

: great introduction

.link https://peter.bourgon.org/ok-log/ OK Log blog post

: drew lots of inspiration, particularly on rationalising coordination between nodes in the cluster

.link https://www.somethingsimilar.com/2013/01/14/notes-on-distributed-systems-for-young-bloods/ Notes on Distributed Systems for Young Bloods
.link https://www.youtube.com/watch?v=1-3Ahy7Fxsc Achieving Rapid Response Times in Large Online Services
.link https://jepsen.io/talks Jepsen distributed systems safety research
.link https://fabxc.org/tsdb/ Writing a Time Series Database from Scratch
.link https://www.qualimente.com/2016/04/26/introduction-to-failure-testing-with-docker/ Failure testing with Docker
.link http://www.vldb.org/pvldb/vol8/p1816-teller.pdf Gorilla: A Fast, Scalable, In-Memory Time Series Database
.link https://www.cs.cornell.edu/~asdas/research/dsn02-swim.pdf SWIM gossip protocol paper
.link https://arxiv.org/abs/1406.2294 Jump hash paper
